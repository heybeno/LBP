{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------------------------------------------------------------------------------\n",
      " Using CLS: 'RBF_SVM' , FE: 'rui-filtered', PP: 'cmask' \n",
      " gamma= 0.000010, C=10\n",
      " Generating model for e_method = 'rui-filtered' , c_method ='RBF_SVM' , work_dir = 'default' \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libavcodec-ffmpeg.so.56: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-018639fae66d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0;31m# Generate model and get score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                 \u001b[0ma_pp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_preproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0ma_pp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_preproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-018639fae66d>\u001b[0m in \u001b[0;36mgenerate_model\u001b[0;34m(self, e_method, c_method, p_method, work_dir)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\" Generating model for e_method = '%s' , c_method ='%s' , work_dir = '%s' \"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# make sure that we have extracted the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;31m# call the classifier with given feature extraction method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_method\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-018639fae66d>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, e_method, p_method, work_dir)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_method\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mpreprocess_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_method\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_model\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rui'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear_SVM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-018639fae66d>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, preprocess_class, work_dir, stages, overwrite)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moverwrite\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libavcodec-ffmpeg.so.56: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "import numpy as np \n",
    "import helper_functions\n",
    "\n",
    "class features_class:\n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "                \n",
    "class image_preprocess:\n",
    "    #Constructor\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        \n",
    "        method: \"pass \" -> no transformation is applied\n",
    "                \"contours\" -> mask input image with contours.\n",
    "                \"cmask\" -> generate mask then apply to features matrix\n",
    "                \n",
    "        thickness : Only used when method is \"contours\" or \"cmask\"\n",
    "    \n",
    "    Functions:\n",
    "        process (image_path, path_to_save)\n",
    "            inputs: image_path -> path for image to process. NOTE: it needs to have \"work\" in the path\n",
    "            output: image_out -> output image. This will also be saved to disk if it does not exist\n",
    "    \"\"\"\n",
    "    def __init__(self, method=\"pass\", params=[]):\n",
    "        self.method   = method\n",
    "        self.params   = params\n",
    "        self.post_process_mask = None\n",
    "        if len(params)>0:\n",
    "            self.thickness = params['thick'] # first param is thickness\n",
    "               \n",
    "    def process (self, image_path):      \n",
    "        binary_threshold = 40\n",
    "        import cv2\n",
    "        import numpy as np\n",
    "        import os\n",
    "\n",
    "        if self.method == \"pass\":\n",
    "            # [ Read image from path and return it unchanged]\n",
    "            image=  cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            return image\n",
    "        elif self.method == \"contours\":\n",
    "            # check if file is already generated\n",
    "            if not \"work\" in image_path:\n",
    "                print \"ERROR: make sure the images have 'work' in the path directory\"\n",
    "                return None\n",
    "            out_filename = image_path.replace(\"work\",\"work_contours_T-\" + str(self.thickness))\n",
    "            if os.path.isfile(out_filename):\n",
    "                # If file exists, simply load it\n",
    "                my_out_img=  cv2.imread(out_filename, cv2.IMREAD_GRAYSCALE)\n",
    "                return my_out_img\n",
    "            else:\n",
    "                # File does not exist, generate it\n",
    "                orig_img   = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                orig_img_clone = orig_img.clone();\n",
    "                elem_open  = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5)) # create a 5x5 processing filter\n",
    "                                                                             # for opening \n",
    "                elem_close = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "                img_open   = cv2.morphologyEx(orig_img,cv2.MORPH_OPEN ,elem_open ) # Image with open morphological op\n",
    "\n",
    "                img_filt        = cv2.morphologyEx(img_open,cv2.MORPH_CLOSE,elem_close) # Closing morphological op\n",
    "                # Threshold to binary\n",
    "                ret, im_bin     = cv2.threshold(img_filt,binary_threshold,1,cv2.THRESH_BINARY)\n",
    "                # Create contours using the binary image\n",
    "                im2, my_contours, hierarchy = cv2.findContours(im_bin.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                # Write a simple contours mask\n",
    "                #print 'self.thickness = ', self.thickness\n",
    "                mask_ext = cv2.drawContours(im_bin.copy(),my_contours,-1,255,self.thickness)\n",
    "                mask = cv2.drawContours(mask_ext.copy(),my_contours,-1,100,2)\n",
    "\n",
    "                # Mask original image:  - Value of 0 where there is no contour\n",
    "                #                       - Pixel value from orig_img if it's on the masked contour\n",
    "                my_masked_orig = np.where((mask_ext==255),orig_img,0)\n",
    "\n",
    "                # Check if the out path is differnt from source ... Otherwise something has gone wrong\n",
    "                if out_filename != image_path:\n",
    "                    print 'Creating: ', out_filename\n",
    "                    base_dir = os.path.split(out_filename)[0]\n",
    "                    if not os.path.isdir(base_dir):\n",
    "                        os.makedirs(base_dir) # create dir if it doesn't exist\n",
    "                    cv2.imwrite(out_filename, my_masked_orig)\n",
    "                # [ Return the image with masked contours]\n",
    "                self.post_process_mask = mask\n",
    "                return orig_img_clone\n",
    "            \n",
    "        elif self.method == \"cmask\":\n",
    "            # -----------\n",
    "            #  This method reads an image, generates a contours mask and\n",
    "            #      - returns the original image (unchanged)\n",
    "            #      - stores the contours mask as a local variable\n",
    "            # -----------\n",
    "            # check if file is already generated\n",
    "            if not \"work\" in image_path:\n",
    "                print \"ERROR: make sure the images have 'work' in the path directory\"\n",
    "                return None\n",
    "            out_filename = image_path.replace(\"work\",\"work_cmask_T-\" + str(self.thickness))\n",
    "            if os.path.isfile(out_filename):\n",
    "                # If file exists, simply load it\n",
    "                self.post_process_mask =  cv2.imread(out_filename, cv2.IMREAD_GRAYSCALE)\n",
    "                orig_img   = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)                \n",
    "                return orig_img\n",
    "            else:\n",
    "                # File does not exist, generate it\n",
    "                orig_img   = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                elem_open  = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5)) # create a 5x5 processing filter\n",
    "                                                                             # for opening \n",
    "                elem_close = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "                img_open   = cv2.morphologyEx(orig_img,cv2.MORPH_OPEN ,elem_open ) # Image with open morphological op\n",
    "\n",
    "                img_filt        = cv2.morphologyEx(img_open,cv2.MORPH_CLOSE,elem_close) # Closing morphological op\n",
    "                # Threshold to binary\n",
    "                ret, im_bin     = cv2.threshold(img_filt,binary_threshold,1,cv2.THRESH_BINARY)\n",
    "                # Create contours using the binary image\n",
    "                im2, my_contours, hierarchy = cv2.findContours(im_bin.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                # Write a simple contours mask\n",
    "                #print 'self.thickness = ', self.thickness\n",
    "                mask_ext = cv2.drawContours(im_bin.copy(),my_contours,-1,255,self.thickness)\n",
    "                mask = cv2.drawContours(mask_ext.copy(),my_contours,-1,100,2)\n",
    "\n",
    "                # Check if the out path is differnt from source ... Otherwise something has gone wrong\n",
    "                if out_filename != image_path:\n",
    "                    print 'Creating: ', out_filename\n",
    "                    base_dir = os.path.split(out_filename)[0]\n",
    "                    if not os.path.isdir(base_dir):\n",
    "                        os.makedirs(base_dir) # create dir if it doesn't exist\n",
    "                    cv2.imwrite(out_filename, mask)\n",
    "                # [ Return the image with masked contours]\n",
    "                return ;   \n",
    "\n",
    "class plant_feature_extraction:\n",
    "    def __init__(self,method,params):\n",
    "        \n",
    "        # Given method can also a features filter argument. We don't want to put the filter\n",
    "        # arguments in our self.params vector as we want the same name file for the extracted\n",
    "        # features. This way, we'll have the same features files for all fitler methods and\n",
    "        # save quite a bit of computation time.\n",
    "        self.method_and_filter = method\n",
    "        self.method = method.split('-filtered')[0] # first split is actual extractor method\n",
    "                \n",
    "        if '-filtered' in self.method_and_filter:\n",
    "            self.features_filter = 'filtered-' + self.method_and_filter.split('-filtered')[1]# second split is fitlter arguments\n",
    "        else:\n",
    "            self.features_filter = 'none'\n",
    "        self.params = params\n",
    "        # ----------------------------\n",
    "        # Internal variables which can change after class instantiation\n",
    "        self.file_base_name = None  # lbp_rui_r1_n8_r2_n16_r3_n24\n",
    "        self.test_file_name = None  # this will be updated when calling initialise()\n",
    "                                    # according to the latest operators\n",
    "        self.train_file_name= None\n",
    "        \n",
    "        self.n_bins         = None\n",
    "        self.e_params       = None\n",
    "        self.is_initialised = None\n",
    "        \n",
    "        self.work_dir       = None\n",
    "        \n",
    "    def initialise(self, preprocessor ,work_dir):   \n",
    "        # Define our dataset training/testing names\n",
    "        self.file_base_name      = 'lbp-' + self.method\n",
    "        # Calculate internal variables according to given methods\n",
    "        pp_base_name='PP-' + preprocessor.method\n",
    "        \n",
    "        for my_operator in  self.params:\n",
    "            self.file_base_name += '-r%d-n%d' %( my_operator[0],my_operator[1])        \n",
    "\n",
    "        for key in preprocessor.params:\n",
    "            pp_base_name += '-' + key + '-' + str(preprocessor.params[key]) # String containing all our params and values\n",
    "                                                                            # so that we don't need to regenerate\n",
    "        self.test_file_name  = 'test_' +'FE-'+ self.file_base_name + '_' + pp_base_name + '.pickle'\n",
    "        self.train_file_name = 'train_' +'FE-'+ self.file_base_name + '_' + pp_base_name + '.pickle'\n",
    "        self.file_base_name +=  pp_base_name         \n",
    "        self.work_dir        = work_dir\n",
    "        self.out_dir         = work_dir.replace('work','out')        \n",
    "        self.is_initialised = 1\n",
    "        \n",
    "    def get_features_histogram (self, image, mask):\n",
    "        import numpy as np\n",
    "        from skimage import feature  \n",
    "        \n",
    "        bins_combined = []\n",
    "        method = self.method.replace('-filtered','') # we don't need the extra -filtered indication here\n",
    "        if method == 'rui':\n",
    "            for r_n_param in self.params:\n",
    "                r = r_n_param[0]\n",
    "                n = r_n_param[1]\n",
    "                my_lbp_matrix  = feature.local_binary_pattern(image, n, r, method=\"uniform\")\n",
    "                masked_lbp_matrix  = np.where((mask>0),my_lbp_matrix,0)\n",
    "                my_bins, my_edges = np.histogram(masked_lbp_matrix.ravel(),bins=np.arange(0, n+3), range=(0,n+2))\n",
    "                bins_combined = np.concatenate( (bins_combined,my_bins) ,axis=0)\n",
    "                #print 'DEBUG: %s , n= %d, r = %d ; n_bins = %d' %(str(r_n_param),n,r,len(bins_combined) )                \n",
    "        self.n_bins = len(bins_combined)\n",
    "        return bins_combined\n",
    "    \n",
    "    def extract_features(self, preprocess_class, work_dir = \"\", stages=['train','test'],overwrite =0):\n",
    "        import os\n",
    "        import cv2\n",
    "        import helper_functions as hlp\n",
    "        import pickle\n",
    "        \n",
    "        if self.is_initialised is None:\n",
    "            self.initialise(preprocess_class, work_dir)\n",
    "            \n",
    "        for my_stage in stages:\n",
    "\n",
    "            if my_stage == 'train':                \n",
    "                feature_extraction_file_name = self.train_file_name\n",
    "                input_directory = work_dir + '/train'\n",
    "            elif my_stage == 'test':\n",
    "                feature_extraction_file_name = self.test_file_name\n",
    "                input_directory = work_dir + '/test'\n",
    "            else:\n",
    "                print 'ERROR !!! uknown stage %s ' % my_stage\n",
    "                return\n",
    "            \n",
    "            my_exising_labels=[x[1] for x in os.walk(input_directory) if len(x[1]) ]\n",
    "            my_exising_labels = my_exising_labels[0]\n",
    "        \n",
    "            if not os.path.isdir(self.out_dir):\n",
    "                os.mkdir(self.out_dir)\n",
    "            out_file_name = self.out_dir + '/' + feature_extraction_file_name\n",
    "            \n",
    "            # Check if file already exists\n",
    "            if not (os.path.isfile(out_file_name) and (overwrite ==0)):\n",
    "                with open(out_file_name, 'wb') as f:\n",
    "                    my_bins_combined = list()\n",
    "                    my_labels       =  list()\n",
    "                    if not os.path.isdir(input_directory):\n",
    "                        print \" ERROR !!! Given input directory '%s' does not exist !!!! \" % input_directory\n",
    "                        return\n",
    "                    print ' ---------------------------------------------------------'\n",
    "                    print \" Creating new feature file: '%s' for %s stage \" % (out_file_name, my_stage)\n",
    "                    for my_label in my_exising_labels:\n",
    "                        print ' LOOKING FOR %s class ' %(my_label)\n",
    "                        my_file_list = hlp.list_files(input_directory + '/' + my_label,file_ext='bmp')\n",
    "                        print '   ... found %d files' %(len(my_file_list))\n",
    "\n",
    "                        for my_file in my_file_list:\n",
    "                            img_path = input_directory + '/' + my_label + '/'+ my_file\n",
    "                            image = preprocess_class.process(img_path)\n",
    "                            bins_combined = self.get_features_histogram(image, preprocess_class.post_process_mask)\n",
    "                            my_bins_combined.append(bins_combined)\n",
    "                            my_labels.append(my_label)\n",
    "\n",
    "                    my_features = features_class(X=my_bins_combined, y=my_labels)\n",
    "                    pickle.dump(my_features,f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def filter_features(self,my_features):\n",
    "        # Decide on some sort of features filter\n",
    "        if self.features_filter !='None':\n",
    "            X = np.delete(my_features.X,[8,26,52],1)\n",
    "            #for i in range(len(my_features.X)): # apply some scaling?\n",
    "            #    temp = my_features.X[i]\n",
    "            #    X[i] *= float(temp[8])/(228*228);     \n",
    "            my_features.X = X\n",
    "        return my_features\n",
    "    def read_features(self,filename):\n",
    "        import pickle\n",
    "        my_features = None\n",
    "        with open (filename,'rb') as f:\n",
    "            my_features = pickle.load(f)\n",
    "                                 \n",
    "        return self.filter_features(my_features) \n",
    "                                 \n",
    "    def read_train_features(self):\n",
    "        train_full_file_name = self.out_dir + '/' + self.train_file_name\n",
    "        return self.read_features(train_full_file_name)\n",
    "                                 \n",
    "    def read_test_features(self):\n",
    "        test_full_file_name = self.out_dir + '/' + self.test_file_name\n",
    "        return self.read_features(test_full_file_name)\n",
    "                    \n",
    "class plant_classifier:\n",
    "    def __init__(self,method,params):\n",
    "        from sklearn.svm import SVC\n",
    "        \n",
    "        self.method = method\n",
    "        self.params = params\n",
    "        \n",
    "        self.out_dir        = None\n",
    "        self.is_initialised = None\n",
    "        \n",
    "        if self.method == 'linear_SVM':\n",
    "                self.my_classifier = SVC( kernel = self.params['kernel'],\n",
    "                                          C      = self.params['C'],\n",
    "                                          gamma  = self.params['gamma']\n",
    "                                        )\n",
    "        \n",
    "        if self.method == 'poly_SVM':\n",
    "                self.my_classifier = SVC( kernel= self.params['kernel'],\n",
    "                                          C     = self.params['C'],\n",
    "                                          gamma = self.params['gamma'],\n",
    "                                          degree= self.params['degree']\n",
    "                                        )\n",
    "            \n",
    "        if self.method == 'RBF_SVM':\n",
    "                self.my_classifier = SVC(kernel = self.params['kernel'],\n",
    "                                         C      = self.params['C'] ,\n",
    "                                         gamma  = self.params['gamma'])\n",
    "                \n",
    "    def initialise(self, feature_extractor):   \n",
    "        # Define our dataset training/testing names\n",
    "        model_base_name ='CLS'\n",
    "        for key in self.params:\n",
    "            model_base_name += '-' + key + '-' + str(self.params[key]) # String containing all our params and values\n",
    "                                                                 # so that we don't need to regenerate\n",
    "        self.file_base_name = model_base_name\n",
    "        # Append feature extractor name ... generate name with filter if any\n",
    "        feature_extractor_name = feature_extractor.file_base_name.replace(feature_extractor.method,\n",
    "                                                                          feature_extractor.method_and_filter)\n",
    "        self.file_base_name += '_FE-' + feature_extractor_name\n",
    "        self.out_dir         =  feature_extractor.work_dir.replace('work','out')\n",
    "\n",
    "        self.model_file_name = 'model_' + self.file_base_name + '.pickle'\n",
    "        self.model_full_file_name =  self.out_dir + '/'+ self.model_file_name\n",
    "        self.global_results_file = self.out_dir + '/' + 'global_results.txt'\n",
    "        \n",
    "        self.is_initialised = 1\n",
    "        \n",
    "        \n",
    "    def train(self, feature_extractor, overwrite=0 ):\n",
    "        import os\n",
    "        import numpy as np\n",
    "        #from sklearn.externals import joblib\n",
    "        import pickle\n",
    "        #from sklearn.svm import SVC\n",
    "        \n",
    "        if feature_extractor.is_initialised is None:\n",
    "            print ' Please initialise the feature extractor first before calling the classifier'\n",
    "            return\n",
    "        \n",
    "        self.initialise(feature_extractor)\n",
    "        if not os.path.isdir(self.out_dir):\n",
    "            os.mkdir(self.out_dir)\n",
    "            print \" Created directory '%s' \" % self.out_dir       \n",
    "        #print 'DEBUG: my model name = %s' %(self.model_file_name)\n",
    "        model_full_file_name = self.model_full_file_name\n",
    "    \n",
    "        \n",
    "        # Check if the model file name already exists\n",
    "        if (os.path.isfile(model_full_file_name) and (not overwrite) )  :\n",
    "            #print \" Model file '%s' already exists. NOT OVERWRITING ! \" % (model_full_file_name) \n",
    "            return\n",
    "        else:\n",
    "            print\" Creating a prediction model file: '%s' \" % model_full_file_name\n",
    "\n",
    "            my_features = feature_extractor.read_train_features()\n",
    "            model = self.my_classifier.fit(my_features.X, my_features.y)\n",
    "            filename = model_full_file_name\n",
    "            print 'Writting file ...'\n",
    "            with open(model_full_file_name, 'wb') as f:\n",
    "                pickle.dump(model,f, pickle.HIGHEST_PROTOCOL)           \n",
    "    \n",
    "    def predict(self, feature_extractor ):\n",
    "        import os\n",
    "        import numpy as np\n",
    "        from sklearn.externals import joblib\n",
    "        import pickle \n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        from sklearn.metrics import classification_report\n",
    "        from natsort import natsorted\n",
    "        \n",
    "        \n",
    "        if feature_extractor.is_initialised is None:\n",
    "            print ' Please initialise the feature extractor first before calling the classifier'\n",
    "            return\n",
    "        \n",
    "        self.initialise(feature_extractor)\n",
    "        model_full_file_name = self.model_full_file_name\n",
    "        \n",
    "        # Check if the model file name already exists\n",
    "        if (not os.path.isfile(model_full_file_name) )  :\n",
    "            print \"Model'%s' does not exist!. Please train model first! \" % (model_full_file_name)\n",
    "            return\n",
    "        \n",
    "        my_features = feature_extractor.read_test_features()\n",
    "            \n",
    "        test_full_file_name = self.out_dir + '/' + feature_extractor.test_file_name\n",
    "                \n",
    "            \n",
    "        with open (test_full_file_name,'rb') as f:\n",
    "            my_features = pickle.load(f)\n",
    "            #TODO: put this in some features_filter ----->\n",
    "            X = np.delete(my_features.X,[8,26,52],1)\n",
    "            #for i in range(len(my_features.X)):\n",
    "            #   temp = my_features.X[i]\n",
    "            #   X[i] *= float(temp[8])/(228*228);\n",
    "            # <----                    \n",
    "        # Load prediction model\n",
    "        with open (model_full_file_name,'rb') as f:\n",
    "            my_model =pickle.load(f)            \n",
    "            #result = loaded_model.score(X_test, Y_test)\n",
    "            predictions = my_model.predict(X)\n",
    "        \n",
    "        current_score = accuracy_score(my_features.y, predictions)\n",
    "        current_model_name = self.model_file_name\n",
    "        print '======================='\n",
    "        print('Accuracy Score = %f' %(current_score) )\n",
    "        print(confusion_matrix(my_features.y, predictions))\n",
    "        print(classification_report(my_features.y, predictions))\n",
    "        \n",
    "        # ---------------------------\n",
    "        # Generate Global results file -> keep track of previous results\n",
    "        # structure: \"precision result\" \"model which was used for results\"\n",
    "        current_result_line = '%2.4f %s\\n' %(current_score, current_model_name)\n",
    "        \n",
    "        if os.path.isfile(self.global_results_file):\n",
    "            # Previous file exists\n",
    "            with open(self.global_results_file,'r') as f:\n",
    "                result_lines = f.readlines(); # contents of previous results\n",
    "                found_previous_results_for_this_model = False\n",
    "\n",
    "\n",
    "                for i in range(len(result_lines)):\n",
    "                    previous_result_line = result_lines[i].strip() # strip the end of line\n",
    "                    previous_model_name = previous_result_line.split(' ')[1]\n",
    "\n",
    "                    if current_model_name == previous_model_name:\n",
    "                        result_lines[i] = current_result_line\n",
    "                        found_previous_results_for_this_model =True\n",
    "                        break\n",
    "                if not found_previous_results_for_this_model:\n",
    "                    result_lines.append(current_result_line)\n",
    "                # Resort to display in nicer format\n",
    "                sorted_results = natsorted(result_lines, reverse=True)\n",
    "            with open(self.global_results_file,'w') as f:\n",
    "                # Rewrite file with new results\n",
    "                f.writelines(sorted_results)                \n",
    "        else:\n",
    "            with open(self.global_results_file,'w') as f:\n",
    "                f.writelines(current_result_line)\n",
    "        return current_score\n",
    "                              \n",
    "class plant_detection:\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, work_dir ='/scratch/data/'):       \n",
    "        self.default_work_dir    = work_dir      \n",
    "        \n",
    "        # Define our Feature extraction methods\n",
    "        self.e_method    = [ \"rui\",\n",
    "                             \"rui-filtered\"]\n",
    "        self.e_params    = [ [(1,8),(2,16),(3,24)],\n",
    "                             [(1,8),(2,16),(3,24)]\n",
    "                           ]\n",
    "    \n",
    "        # Define our Classification methods\n",
    "        self.c_method = [ \"linear_SVM\",  \n",
    "                          \"poly_SVM\"  ,\n",
    "                          \"RBF_SVM\" ,   \n",
    "                        ]\n",
    "        self.c_params = [# Params for linear_SVM\n",
    "                          dict([('kernel','linear' ),\n",
    "                               ('C'     , 10      ),\n",
    "                               ('gamma' ,0.000001 )]),\n",
    "                          # Params for poly_SVM\n",
    "                          dict([('kernel','poly'),\n",
    "                               ('C'     , 10      ),\n",
    "                               ('gamma' ,0.000001 ),\n",
    "                               ('degree',2        )]),\n",
    "                          # Params for RBF_SVM\n",
    "                          dict([('kernel','rbf'    ),\n",
    "                               ('C'     , 40      ),\n",
    "                               ('gamma' ,0.00001 )]),\n",
    "                         ]\n",
    "        # Define our Image pre-processing methods\n",
    "        self.p_method    = [ \"pass\",\n",
    "                             \"contours\",\n",
    "                             \"cmask\"]\n",
    "        self.p_params    = [ dict(),\n",
    "                             dict([('thick',6)]),\n",
    "                             dict([('thick',6)])\n",
    "                           ]\n",
    "        # create dictionaries (for easier indexing)\n",
    "        self.e_dict = dict( [ ( self.e_method[i],i ) for i in range(len(self.e_method)) ] )\n",
    "        self.c_dict = dict( [ ( self.c_method[i],i ) for i in range(len(self.c_method)) ] )     \n",
    "        self.p_dict = dict( [ ( self.p_method[i],i ) for i in range(len(self.p_method)) ] )\n",
    "        \n",
    "        self.initialized = False\n",
    "    \n",
    "    def initialize(self):\n",
    "        # Create Feature Extractors\n",
    "        self.e_list   = list()\n",
    "        for i in range (len(self.e_method)):\n",
    "            self.e = plant_feature_extraction(self.e_method[i],self.e_params[i])\n",
    "            self.e_list.extend ([self.e] )\n",
    "            \n",
    "        # Create Classifiers\n",
    "        self.c_list   = list()\n",
    "        for i in range (len(self.c_method)):\n",
    "            self.c = plant_classifier(self.c_method[i],self.c_params[i])\n",
    "            self.c_list.extend ([self.c] )\n",
    "            \n",
    "        # Create Preprocessing classes\n",
    "        self.p_list      = list()\n",
    "        for i in range (len(self.p_method)):\n",
    "            self.p = image_preprocess(self.p_method[i],self.p_params[i])\n",
    "            self.p_list.extend ([self.p] )\n",
    "        \n",
    "        self.initialized = True\n",
    "                                    \n",
    "    def extract_features(self, e_method='rui', p_method='pass', work_dir='default'):\n",
    "        if work_dir == 'default':\n",
    "            work_dir = self.default_work_dir\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        # Define our dataset training/testing names\n",
    "        self.e = self.e_list[self.e_dict[e_method]]\n",
    "        preprocess_class = self.p_list[self.p_dict[p_method]]\n",
    "        self.e.extract_features(preprocess_class, work_dir)\n",
    "        \n",
    "    def generate_model (self, e_method='rui', c_method='linear_SVM', p_method='pass', work_dir='default'):\n",
    "        print \" Generating model for e_method = '%s' , c_method ='%s' , work_dir = '%s' \" %(e_method,c_method, work_dir)\n",
    "        # make sure that we have extracted the features\n",
    "        self.extract_features(e_method,p_method, work_dir)    \n",
    "        # call the classifier with given feature extraction method\n",
    "        self.c = self.c_list[self.c_dict[c_method]]\n",
    "        self.c.train(self.e)\n",
    "        \n",
    "         \n",
    "    def prediction (self, e_method ='rui', c_method='linear_SVM', p_method='pass', out_dir='default'):\n",
    "        print \" Prediction model for e_method = '%s' , c_method ='%s' , out_dir = '%s' \" %(e_method, c_method, out_dir)\n",
    "        # make sure that we have extracted the features\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        self.e = self.e_list[self.e_dict[e_method]]\n",
    "        self.extract_features(e_method, p_method, self.e.work_dir)    \n",
    "        # call the classifier with given feature extraction method\n",
    "        self.c = self.c_list[self.c_dict[c_method]]\n",
    "        self.c.predict(self.e)\n",
    "\n",
    "# -----------------------------------------------        \n",
    "# The above will go into a module when finalised.\n",
    "# The code below is an example of how to use it\n",
    "# -----------------------------------------------        \n",
    "\n",
    "work_dir = \"/scratch/git/LBP/data/small_dataset/work/\"\n",
    "my_extractor  = 'rui-filtered'\n",
    "my_classifiers = ['RBF_SVM']#, 'linear_SVM']\n",
    "my_preprocess  = ['cmask']#,'pass']\n",
    "my_C_list = [10]#, 20, 30]\n",
    "my_gamma_list= [1e-5]#,1e-6 ]\n",
    "\n",
    "for my_classifier in my_classifiers:\n",
    "    for my_preproc in my_preprocess:\n",
    "        for my_C in my_C_list:\n",
    "            for my_gamma in my_gamma_list:\n",
    "                print ' ------------------------------------------------------------------------------'\n",
    "                print \" Using CLS: '%s' , FE: '%s', PP: '%s' \" %(my_classifier, my_extractor, my_preproc)\n",
    "                print \" gamma= %f, C=%d\" %(my_gamma,my_C)\n",
    "\n",
    "                a_pp = plant_detection(work_dir = work_dir)\n",
    "                \n",
    "                # override the default C and gamma from the classifer definition\n",
    "                classifier_params_index = a_pp.c_dict[my_classifier]\n",
    "                (a_pp.c_params[classifier_params_index])['gamma'] = my_gamma\n",
    "                (a_pp.c_params[classifier_params_index])['C']     = my_C                                                   \n",
    "                                                      \n",
    "                # Generate model and get score\n",
    "                a_pp.generate_model(e_method=my_extractor, c_method=my_classifier, p_method=my_preproc)\n",
    "                a_pp.prediction(e_method=my_extractor, c_method=my_classifier, p_method=my_preproc)\n",
    "\n",
    "print 'ALL DONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "a = \"aagiven_string-filtered-k0\"\n",
    "a.split('-filtered')\n",
    "b = 1.9\n",
    "my_new_line= '%2.2f %s\\n' %(b,a)\n",
    "#c=list()\n",
    "c.append(my_new_line)\n",
    "    \n",
    "print c\n",
    "print natsorted(c,reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pp = plant_detection(work_dir = work_dir)\n",
    "classifier_params_index = a_pp.c_dict[my_classifier]\n",
    "print classifier_params_index\n",
    "(a_pp.c_params[2])['gamma'] = my_gamma\n",
    "(a_pp.c_params[2])['gamma']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
